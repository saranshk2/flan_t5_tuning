{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c65f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ae478",
   "metadata": {},
   "source": [
    "### 1. Use a pre-trained google/flan-t5-small as the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a37cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google_flan_t5/\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15c5d7",
   "metadata": {},
   "source": [
    "### 2. Verify if the summarizaton task works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd2a5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.\n"
     ]
    }
   ],
   "source": [
    "input_text = '''Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language\n",
    "                        processing (NLP). The effectiveness of transfer learning has given rise to a diversity of\n",
    "                        approaches, methodology, and practice. In this paper, we explore the landscape of transfer\n",
    "                        learning techniques for NLP by introducing a unified framework that converts all text-based\n",
    "                        language problems into a text-to-text format. Our systematic study compares pre-training\n",
    "                        objectives, architectures, unlabeled data sets, transfer approaches, and other factors on\n",
    "                        dozens of language understanding tasks. By combining the insights from our exploration\n",
    "                        with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results\n",
    "                        on many benchmarks covering summarization, question answering, text classification, and\n",
    "                        more. To facilitate future work on transfer learning for NLP, we release our data set,\n",
    "                        pre-trained models, and code.'''\n",
    "                        \n",
    "inputs = tokenizer.encode('''summarize: ''' + input_text,\n",
    "                          return_tensors='pt',\n",
    "                          max_length=512,\n",
    "                          truncation=True)\n",
    "summarization_ids = model.generate(inputs, max_length=120, min_length=40, num_beams=4)\n",
    "summarization = tokenizer.decode(summarization_ids[0], skip_special_tokens = True)\n",
    "print(summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad8460",
   "metadata": {},
   "source": [
    "### 3. Verify if the Q&A task works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530d0462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n"
     ]
    }
   ],
   "source": [
    "question = \"What year did the Berlin Wall fall?\"\n",
    "context = \"The Berlin Wall, a significant symbol of the Cold War, was finally brought down in 1989, leading to the reunification of East and West Germany\"\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512,truncation=True)\n",
    "qa_ids = model.generate(inputs, max_length=50)\n",
    "answer = tokenizer.decode(qa_ids[0], skip_special_tokens = True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f074c",
   "metadata": {},
   "source": [
    "### 4. Verify if English to French translation task works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51cc6056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour faciliter la mise en uvre de l'apprentissage de transfert, nous remettent à l'élaboration de nos données.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"To facilitate future work on transfer learning, we release our data set\"\n",
    "\n",
    "inputs = tokenizer.encode(\"translate English to French: \"+ input_text, return_tensors=\"pt\", max_length=512,truncation=True)\n",
    "language_ids = model.generate(inputs, max_length=50)\n",
    "language_translation = tokenizer.decode(language_ids[0], skip_special_tokens = True)\n",
    "print(language_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064841c9",
   "metadata": {},
   "source": [
    "### 5. Programmatically print the names of all the model layers and their dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd02cd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: shared.weight, Dimension: torch.Size([32128, 512])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Dimension: torch.Size([32, 6])\n",
      "Layer Name: encoder.block.0.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.0.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.0.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.0.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.0.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.1.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.1.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.1.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.1.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.1.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.1.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.1.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.1.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.1.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.2.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.2.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.2.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.2.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.2.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.2.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.2.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.2.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.2.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.3.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.3.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.3.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.3.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.3.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.3.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.3.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.3.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.3.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.4.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.4.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.4.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.4.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.4.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.4.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.4.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.4.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.4.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.5.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.5.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.5.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.5.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.5.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.5.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.5.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.5.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.5.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.6.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.6.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.6.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.6.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.6.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.6.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.6.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.6.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.6.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.7.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.7.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.7.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: encoder.block.7.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: encoder.block.7.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.block.7.layer.1.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.7.layer.1.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: encoder.block.7.layer.1.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: encoder.block.7.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: encoder.final_layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight, Dimension: torch.Size([32, 6])\n",
      "Layer Name: decoder.block.0.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.0.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.0.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.0.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.0.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.0.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.0.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.0.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.1.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.1.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.1.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.1.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.1.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.1.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.1.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.1.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.1.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.2.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.2.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.2.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.2.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.2.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.2.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.2.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.2.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.2.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.3.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.3.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.3.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.3.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.3.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.3.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.3.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.3.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.3.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.4.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.4.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.4.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.4.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.4.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.4.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.4.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.4.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.4.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.5.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.5.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.5.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.5.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.5.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.5.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.5.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.5.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.5.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.6.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.6.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.6.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.6.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.6.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.6.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.6.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.6.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.6.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.7.layer.0.SelfAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.0.SelfAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.0.SelfAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.0.SelfAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.7.layer.0.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.7.layer.1.EncDecAttention.q.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.1.EncDecAttention.k.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.1.EncDecAttention.v.weight, Dimension: torch.Size([384, 512])\n",
      "Layer Name: decoder.block.7.layer.1.EncDecAttention.o.weight, Dimension: torch.Size([512, 384])\n",
      "Layer Name: decoder.block.7.layer.1.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.block.7.layer.2.DenseReluDense.wi_0.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.7.layer.2.DenseReluDense.wi_1.weight, Dimension: torch.Size([1024, 512])\n",
      "Layer Name: decoder.block.7.layer.2.DenseReluDense.wo.weight, Dimension: torch.Size([512, 1024])\n",
      "Layer Name: decoder.block.7.layer.2.layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: decoder.final_layer_norm.weight, Dimension: torch.Size([512])\n",
      "Layer Name: lm_head.weight, Dimension: torch.Size([32128, 512])\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(f\"Layer Name: {name}, Dimension: {params.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a7093",
   "metadata": {},
   "source": [
    "### 6. Programmatically print the total number of parameters/weights in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8491243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76961152\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69b2b0",
   "metadata": {},
   "source": [
    "### 7. Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e032fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.final_layer_norm.weight.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c10ce4",
   "metadata": {},
   "source": [
    "### 8. Verify if the Q&A task works after resettng the weights of the above layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0658c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "question = \"What year did the Berlin Wall fall?\"\n",
    "context = \"The Berlin Wall, a significant symbol of the Cold War, was finally brought down in 1989, leading to the reunification of East and West Germany\"\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512,truncation=True)\n",
    "qa_ids = model.generate(inputs, max_length=50)\n",
    "answer = tokenizer.decode(qa_ids[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf1859",
   "metadata": {},
   "source": [
    "### 10. Train the model for a Q&A task that takes a context as additional input along with the question. You can use SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d4aaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google_flan_t5/\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205ecf2",
   "metadata": {},
   "source": [
    "#### Training_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8053657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_parquet('squad/train-00000-of-00001.parquet').sample(1000)\n",
    "\n",
    "train_dataset['answers_text'] = train_dataset['answers'].apply(lambda x : x['text'][0] if x['text'] else '')\n",
    "train_dataset['input_text'] = \"question: \" + train_dataset['question'] + ' context: ' + train_dataset['context']\n",
    "train_dataset = train_dataset[train_dataset.answers_text.str.len() > 0 ]\n",
    "\n",
    "input_encodings = tokenizer(train_dataset.input_text.tolist(), return_tensors=\"pt\", max_length=512,truncation=True, padding = True)\n",
    "target_encodings = tokenizer(train_dataset.answers_text.tolist(), return_tensors=\"pt\", max_length=512,truncation=True, padding = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53715368",
   "metadata": {},
   "source": [
    "#### Test_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "616bf78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans(text):\n",
    "    try:\n",
    "        return text['text'][0]\n",
    "    except:\n",
    "        return ''\n",
    "test_dataset = pd.read_parquet('squad/validation-00000-of-00001.parquet').sample(100)\n",
    "test_dataset['input_text'] = \"question: \" + test_dataset['question'] + ' context: ' + test_dataset['context']\n",
    "test_input_encodings = tokenizer(test_dataset.input_text.tolist(), return_tensors=\"pt\", max_length=512,truncation=True, padding = True)\n",
    "test_dataset['answers_text'] = test_dataset['answers'].apply(ans)\n",
    "truths = test_dataset['answers_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc22ced",
   "metadata": {},
   "source": [
    "#### Benchmarking without Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e64b5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = {k: v for k, v in test_input_encodings.items()}\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=20)\n",
    "    \n",
    "predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e046998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match (EM): 0.43\n",
      "Average F1 Score: 0.59\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punct(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_punct(lower(s)))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "\n",
    "em_scores = [exact_match_score(pred, ans) for pred, ans in zip(predictions, truths)]\n",
    "f1_scores = [f1_score(pred, ans) for pred, ans in zip(predictions, truths)]\n",
    "\n",
    "average_em = sum(em_scores) / len(em_scores)\n",
    "average_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print(f\"Average Exact Match (EM): {average_em:.2f}\")\n",
    "print(f\"Average F1 Score: {average_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f7321",
   "metadata": {},
   "source": [
    "#### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d40a356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SQuADT5Dataset(Dataset):\n",
    "    def __init__(self, input_encodings, target_encodings):\n",
    "        self.input_encodings = input_encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.input_encodings.items()}\n",
    "        item['labels'] = target_encodings['input_ids'][idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04f0d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SQuADT5Dataset(input_encodings, target_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e76da9",
   "metadata": {},
   "source": [
    "#### Freezing all layers except last two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37dd57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "for parameter in model.decoder.final_layer_norm.parameters():\n",
    "    parameter.requires_grad = True\n",
    "    \n",
    "for parameter in model.lm_head.parameters():\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c201d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_layer_trainable_status(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {'trainable' if param.requires_grad else 'frozen'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d007c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight: frozen\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight: frozen\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight: frozen\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight: frozen\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight: frozen\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: frozen\n",
      "encoder.block.0.layer.0.layer_norm.weight: frozen\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight: frozen\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight: frozen\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight: frozen\n",
      "encoder.block.0.layer.1.layer_norm.weight: frozen\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight: frozen\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight: frozen\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight: frozen\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight: frozen\n",
      "encoder.block.1.layer.0.layer_norm.weight: frozen\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight: frozen\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight: frozen\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight: frozen\n",
      "encoder.block.1.layer.1.layer_norm.weight: frozen\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight: frozen\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight: frozen\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight: frozen\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight: frozen\n",
      "encoder.block.2.layer.0.layer_norm.weight: frozen\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight: frozen\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight: frozen\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight: frozen\n",
      "encoder.block.2.layer.1.layer_norm.weight: frozen\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight: frozen\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight: frozen\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight: frozen\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight: frozen\n",
      "encoder.block.3.layer.0.layer_norm.weight: frozen\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight: frozen\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight: frozen\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight: frozen\n",
      "encoder.block.3.layer.1.layer_norm.weight: frozen\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight: frozen\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight: frozen\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight: frozen\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight: frozen\n",
      "encoder.block.4.layer.0.layer_norm.weight: frozen\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight: frozen\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight: frozen\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight: frozen\n",
      "encoder.block.4.layer.1.layer_norm.weight: frozen\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight: frozen\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight: frozen\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight: frozen\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight: frozen\n",
      "encoder.block.5.layer.0.layer_norm.weight: frozen\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight: frozen\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight: frozen\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight: frozen\n",
      "encoder.block.5.layer.1.layer_norm.weight: frozen\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight: frozen\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight: frozen\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight: frozen\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight: frozen\n",
      "encoder.block.6.layer.0.layer_norm.weight: frozen\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight: frozen\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight: frozen\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight: frozen\n",
      "encoder.block.6.layer.1.layer_norm.weight: frozen\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight: frozen\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight: frozen\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight: frozen\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight: frozen\n",
      "encoder.block.7.layer.0.layer_norm.weight: frozen\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight: frozen\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight: frozen\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight: frozen\n",
      "encoder.block.7.layer.1.layer_norm.weight: frozen\n",
      "encoder.final_layer_norm.weight: frozen\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight: frozen\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight: frozen\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight: frozen\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight: frozen\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: frozen\n",
      "decoder.block.0.layer.0.layer_norm.weight: frozen\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight: frozen\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight: frozen\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight: frozen\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight: frozen\n",
      "decoder.block.0.layer.1.layer_norm.weight: frozen\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0.weight: frozen\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1.weight: frozen\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight: frozen\n",
      "decoder.block.0.layer.2.layer_norm.weight: frozen\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight: frozen\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight: frozen\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight: frozen\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight: frozen\n",
      "decoder.block.1.layer.0.layer_norm.weight: frozen\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight: frozen\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight: frozen\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight: frozen\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight: frozen\n",
      "decoder.block.1.layer.1.layer_norm.weight: frozen\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0.weight: frozen\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1.weight: frozen\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight: frozen\n",
      "decoder.block.1.layer.2.layer_norm.weight: frozen\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight: frozen\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight: frozen\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight: frozen\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight: frozen\n",
      "decoder.block.2.layer.0.layer_norm.weight: frozen\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight: frozen\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight: frozen\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight: frozen\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight: frozen\n",
      "decoder.block.2.layer.1.layer_norm.weight: frozen\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_0.weight: frozen\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_1.weight: frozen\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight: frozen\n",
      "decoder.block.2.layer.2.layer_norm.weight: frozen\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight: frozen\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight: frozen\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight: frozen\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight: frozen\n",
      "decoder.block.3.layer.0.layer_norm.weight: frozen\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight: frozen\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight: frozen\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight: frozen\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight: frozen\n",
      "decoder.block.3.layer.1.layer_norm.weight: frozen\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_0.weight: frozen\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_1.weight: frozen\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight: frozen\n",
      "decoder.block.3.layer.2.layer_norm.weight: frozen\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight: frozen\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight: frozen\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight: frozen\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight: frozen\n",
      "decoder.block.4.layer.0.layer_norm.weight: frozen\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight: frozen\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight: frozen\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight: frozen\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight: frozen\n",
      "decoder.block.4.layer.1.layer_norm.weight: frozen\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_0.weight: frozen\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_1.weight: frozen\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight: frozen\n",
      "decoder.block.4.layer.2.layer_norm.weight: frozen\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight: frozen\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight: frozen\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight: frozen\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight: frozen\n",
      "decoder.block.5.layer.0.layer_norm.weight: frozen\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight: frozen\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight: frozen\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight: frozen\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight: frozen\n",
      "decoder.block.5.layer.1.layer_norm.weight: frozen\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_0.weight: frozen\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_1.weight: frozen\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight: frozen\n",
      "decoder.block.5.layer.2.layer_norm.weight: frozen\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight: frozen\n",
      "decoder.block.6.layer.0.SelfAttention.k.weight: frozen\n",
      "decoder.block.6.layer.0.SelfAttention.v.weight: frozen\n",
      "decoder.block.6.layer.0.SelfAttention.o.weight: frozen\n",
      "decoder.block.6.layer.0.layer_norm.weight: frozen\n",
      "decoder.block.6.layer.1.EncDecAttention.q.weight: frozen\n",
      "decoder.block.6.layer.1.EncDecAttention.k.weight: frozen\n",
      "decoder.block.6.layer.1.EncDecAttention.v.weight: frozen\n",
      "decoder.block.6.layer.1.EncDecAttention.o.weight: frozen\n",
      "decoder.block.6.layer.1.layer_norm.weight: frozen\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_0.weight: frozen\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_1.weight: frozen\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight: frozen\n",
      "decoder.block.6.layer.2.layer_norm.weight: frozen\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight: frozen\n",
      "decoder.block.7.layer.0.SelfAttention.k.weight: frozen\n",
      "decoder.block.7.layer.0.SelfAttention.v.weight: frozen\n",
      "decoder.block.7.layer.0.SelfAttention.o.weight: frozen\n",
      "decoder.block.7.layer.0.layer_norm.weight: frozen\n",
      "decoder.block.7.layer.1.EncDecAttention.q.weight: frozen\n",
      "decoder.block.7.layer.1.EncDecAttention.k.weight: frozen\n",
      "decoder.block.7.layer.1.EncDecAttention.v.weight: frozen\n",
      "decoder.block.7.layer.1.EncDecAttention.o.weight: frozen\n",
      "decoder.block.7.layer.1.layer_norm.weight: frozen\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_0.weight: frozen\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_1.weight: frozen\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight: frozen\n",
      "decoder.block.7.layer.2.layer_norm.weight: frozen\n",
      "decoder.final_layer_norm.weight: trainable\n",
      "lm_head.weight: trainable\n"
     ]
    }
   ],
   "source": [
    "print_layer_trainable_status(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a480c",
   "metadata": {},
   "source": [
    "#### Trainig_Loop - Only 1 epoch with Batch Size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "517ffb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4796ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 1... Loss: 43.167633...\n",
      "Epoch: 1/1... Step: 2... Loss: 43.115242...\n",
      "Epoch: 1/1... Step: 3... Loss: 42.771740...\n",
      "Epoch: 1/1... Step: 4... Loss: 43.050915...\n",
      "Epoch: 1/1... Step: 5... Loss: 42.636387...\n",
      "Epoch: 1/1... Step: 6... Loss: 40.962044...\n",
      "Epoch: 1/1... Step: 7... Loss: 40.944424...\n",
      "Epoch: 1/1... Step: 8... Loss: 43.161983...\n",
      "Epoch: 1/1... Step: 9... Loss: 41.652943...\n",
      "Epoch: 1/1... Step: 10... Loss: 43.153229...\n",
      "Epoch: 1/1... Step: 11... Loss: 43.107143...\n",
      "Epoch: 1/1... Step: 12... Loss: 43.187603...\n",
      "Epoch: 1/1... Step: 13... Loss: 42.171211...\n",
      "Epoch: 1/1... Step: 14... Loss: 41.593601...\n",
      "Epoch: 1/1... Step: 15... Loss: 42.631622...\n",
      "Epoch: 1/1... Step: 16... Loss: 42.248489...\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "print_every = 1\n",
    "model.train()\n",
    "for e in range(num_epochs):\n",
    "    counter = 0\n",
    "    for batch in train_loader:\n",
    "        counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if counter % print_every == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, num_epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81e4630b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69942ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = {k: v for k, v in test_input_encodings.items()}\n",
    "    outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=20)\n",
    "    \n",
    "predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abaf3ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match (EM): 0.43\n",
      "Average F1 Score: 0.59\n"
     ]
    }
   ],
   "source": [
    "em_scores = [exact_match_score(pred, ans) for pred, ans in zip(predictions, truths)]\n",
    "f1_scores = [f1_score(pred, ans) for pred, ans in zip(predictions, truths)]\n",
    "\n",
    "average_em = sum(em_scores) / len(em_scores)\n",
    "average_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print(f\"Average Exact Match (EM): {average_em:.2f}\")\n",
    "print(f\"Average F1 Score: {average_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c700e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
